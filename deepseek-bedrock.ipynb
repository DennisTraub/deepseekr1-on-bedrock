{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "    DeepSeek-R1 with Amazon Bedrock \n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "- An AWS account with access to Amazon Bedrock\n",
    "- Sufficient local storage space (at least 17GB for 8B and 135GB for 70B models)\n",
    "- (Optional) An Amazon S3 bucket prepared to store the custom model\n",
    "- (Optional) An AWS IAM Role with permissions for Bedrock to read from S3\n",
    "\n",
    "## Step 1: Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T10:41:51.997883Z",
     "start_time": "2025-02-06T10:41:48.159760Z"
    }
   },
   "source": "!pip install boto3 huggingface_hub transformers[torch] -U",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (1.36.14)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (4.48.2)\n",
      "Requirement already satisfied: botocore<1.37.0,>=1.36.14 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from boto3) (1.36.14)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from boto3) (0.11.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from transformers[torch]) (0.5.2)\n",
      "Requirement already satisfied: torch>=2.0 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from transformers[torch]) (2.6.0)\n",
      "Collecting accelerate>=0.26.0 (from transformers[torch])\n",
      "  Downloading accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (6.1.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from botocore<1.37.0,>=1.36.14->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from botocore<1.37.0,>=1.36.14->boto3) (2.3.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from torch>=2.0->transformers[torch]) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from torch>=2.0->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from requests->huggingface_hub) (2024.12.14)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.37.0,>=1.36.14->boto3) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\traubd\\code\\github\\dennistraub\\deepseekr1-on-bedrock\\.venv\\lib\\site-packages (from jinja2->torch>=2.0->transformers[torch]) (3.0.2)\n",
      "Downloading accelerate-1.3.0-py3-none-any.whl (336 kB)\n",
      "   ---------------------------------------- 0.0/336.6 kB ? eta -:--:--\n",
      "   ------------------------------ --------- 256.0/336.6 kB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 336.6/336.6 kB 6.9 MB/s eta 0:00:00\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.3.0\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure parameters\n",
    "Note: When using the defaults, unique random strings will be appended to resource names to prevent naming conflicts."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T10:38:10.535929Z",
     "start_time": "2025-02-06T10:37:44.401787Z"
    }
   },
   "source": [
    "# Default configuration values\n",
    "default_region = 'us-east-1'\n",
    "default_repository_id = 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B'\n",
    "default_s3_root_folder = '/'\n",
    "default_s3_bucket_base_name = 'bedrock-imported-models'\n",
    "default_import_role_base_name = 'AmazonBedrockModelImportRole'\n",
    "default_import_policy_name = 'AmazonBedrockModelImportPolicy'\n",
    "\n",
    "# Collect required parameters from user\n",
    "# Allow user to specify custom Hugging Face model repository\n",
    "repository_id = input(f\"Enter Hugging Face repository ID ['{default_repository_id}']: \") or default_repository_id\n",
    "\n",
    "# Allow user to specify AWS region for deployment\n",
    "aws_region = input(f\"Enter the AWS region: ['us-east-1']: \") or default_region\n",
    "\n",
    "# Get IAM role name for model import permissions\n",
    "# If left empty, a new role will be created automatically\n",
    "import_role_name = input(\"Enter the IAM role name for the model import [Leave empty to create a new role]\") or None\n",
    "\n",
    "# Get S3 storage configuration\n",
    "# If left is empty, a new bucket will be created\n",
    "s3_bucket_name = input('Enter the S3 bucket name [Leave empty to create a new bucket]') or None\n",
    "s3_root_folder = input(f\"Enter the S3 root prefix ['{default_s3_root_folder}']\") or default_s3_root_folder\n",
    "\n",
    "# Display final configuration settings\n",
    "print('Configuration:')\n",
    "print(f\"- HF Repository ID: {repository_id}\")\n",
    "print(f\"- Import role ARN: {import_role_name or 'Create a new IAM role'}\")\n",
    "print(f\"- S3 bucket: {s3_bucket_name or 'Create a new S3 bucket'}\")\n",
    "print(f\"- S3 root folder: {s3_root_folder}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "- HF Repository ID: deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
      "- Import role ARN: AmazonBedrockModelImportRole-42sjopt3\n",
      "- S3 bucket: bedrock-imported-models-42sjopt3\n",
      "- S3 root folder: /\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create new IAM Role and S3 Bucket if required"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T10:38:17.459657Z",
     "start_time": "2025-02-06T10:38:15.090382Z"
    }
   },
   "source": [
    "import boto3\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a random resource name postfix\n",
    "postfix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n",
    "\n",
    "def get_aws_account_id():\n",
    "    sts_client = boto3.client('sts')\n",
    "    return sts_client.get_caller_identity()['Account']\n",
    "\n",
    "def get_or_create_role(role_name):\n",
    "    iam_client = boto3.client('iam')\n",
    "    \n",
    "    if not role_name:\n",
    "        print('Creating new IAM role...')\n",
    "        \n",
    "        account_id = get_aws_account_id()\n",
    "        role_name = f\"{default_import_role_base_name}-{postfix}\"\n",
    "       \n",
    "        trust_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [{\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": { \"Service\": \"bedrock.amazonaws.com\" },\n",
    "                \"Action\": \"sts:AssumeRole\",\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": { \"aws:SourceAccount\": account_id },\n",
    "                    \"ArnEquals\": {\n",
    "                        \"aws:SourceArn\": f\"arn:aws:bedrock:{aws_region}:{account_id}:model-import-job/*\"\n",
    "                    }\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        inline_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [{\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": [\n",
    "                        \"s3:GetObject\",\n",
    "                        \"s3:ListBucket\"\n",
    "                    ],\n",
    "                    \"Resource\": [\n",
    "                        s3_bucket_arn,\n",
    "                        f\"{s3_bucket_arn}/*\"\n",
    "                    ],\n",
    "                    \"Condition\": {\n",
    "                        \"StringEquals\": {\n",
    "                            \"aws:ResourceAccount\": account_id\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            role = iam_client.create_role(\n",
    "                RoleName=role_name,\n",
    "                AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
    "            )\n",
    "\n",
    "            iam_client.put_role_policy(\n",
    "                RoleName=role_name,\n",
    "                PolicyName=f\"{default_import_policy_name}\",\n",
    "                PolicyDocument=json.dumps(inline_policy)\n",
    "            )\n",
    "            \n",
    "            print(f\"Successfully created IAM role: {role_name}\")\n",
    "        except ClientError as e:\n",
    "            print(f\"Error creating IAM role: {e}\")\n",
    "            exit(1)\n",
    "    else:\n",
    "        print(f\"Checking IAM role: {role_name}\")\n",
    "        try:\n",
    "            role = iam_client.get_role(RoleName=role_name)\n",
    "            print('Found existing role.')\n",
    "        except ClientError as e:\n",
    "            print(f\"Error retrieving S3 bucket: {e}\")\n",
    "            exit(1)\n",
    "            \n",
    "    return role[\"Role\"]\n",
    "\n",
    "def get_or_create_bucket(bucket_name):\n",
    "    s3_client = boto3.client('s3', region_name=aws_region)\n",
    "    \n",
    "    if not bucket_name:\n",
    "        print(f\"Creating new S3 bucket...\")\n",
    "        \n",
    "        bucket_name = f\"{default_s3_bucket_base_name}-{postfix}\"\n",
    "        \n",
    "        try:\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "            \n",
    "            # Wait until bucket exists\n",
    "            waiter = s3_client.get_waiter('bucket_exists')\n",
    "            waiter.wait(\n",
    "                Bucket=bucket_name,\n",
    "                WaiterConfig={\n",
    "                    'Delay': 5,\n",
    "                    'MaxAttempts': 20\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            print(f\"Successfully created S3 bucket: {bucket_name}\")\n",
    "            return bucket_name\n",
    "            \n",
    "        except ClientError as e:\n",
    "            print(f\"Error creating S3 bucket: {e}\")\n",
    "            exit(1)\n",
    "    else:\n",
    "        print(f\"Checking S3 bucket: {bucket_name}\")\n",
    "        try:\n",
    "            bucket = s3_client.head_bucket(Bucket=bucket_name)\n",
    "            print('Found existing bucket')\n",
    "        except ClientError as e:\n",
    "            print(f\"Error retrieving IAM role: {e}\")\n",
    "            exit(1)\n",
    "            \n",
    "s3_bucket_arn = f\"arn:aws:s3:::{s3_bucket_name}\"\n",
    "import_role = get_or_create_role(import_role_name)\n",
    "import_role_arn = import_role[\"Arn\"]\n",
    "\n",
    "s3_bucket = get_or_create_bucket(s3_bucket_name)\n",
    "s3_bucket_arn = f\"arn:aws:s3:::{s3_bucket_name}\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking IAM role: AmazonBedrockModelImportRole-42sjopt3\n",
      "Found existing role.\n",
      "Checking S3 bucket: bedrock-imported-models-42sjopt3\n",
      "Found existing bucket\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Download and deploy the model\n",
    "## Step 1: Download the weights from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T10:38:23.310406Z",
     "start_time": "2025-02-06T10:38:21.949569Z"
    }
   },
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_dir = snapshot_download(repository_id)\n",
    "print(f\"Model downloaded to: {local_dir}\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b71eb7c632b64c9cba9b5c358ec4a450"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/19.0k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d839eae4b2a459da9ea1cae4497ddb3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded to: C:\\Users\\traubd\\.cache\\huggingface\\hub\\models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B\\snapshots\\ebf7e8d03db3d86a442d22d30d499abb7ec27bea\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload the weights to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T10:38:32.308133Z",
     "start_time": "2025-02-06T10:38:29.933186Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "s3_folder = repository_id if s3_root_folder == '/' else f'{s3_root_folder}/{repository_id}'\n",
    "s3_folder_uri = f\"s3://{s3_bucket_name}/{s3_folder}\"\n",
    "\n",
    "def file_exists_in_s3(bucket_name, s3_key):\n",
    "    return s3.list_objects_v2(Bucket=bucket_name, Prefix=s3_key)['KeyCount'] > 0\n",
    "\n",
    "def upload_to_s3():\n",
    "    for root, _, files in os.walk(local_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(file_path, local_dir)\n",
    "            s3_key = f\"{s3_folder}/{relative_path}\"\n",
    "            \n",
    "            if file_exists_in_s3(s3_bucket_name, s3_key):\n",
    "                print(f\"Skipping existing file: s3://{s3_bucket_name}/{s3_key}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Uploading: {file_path} to s3://{s3_bucket_name}/{s3_key}\")\n",
    "            s3.upload_file(file_path, s3_bucket_name, s3_key)\n",
    "\n",
    "print('Uploading model files to S3...')\n",
    "\n",
    "upload_to_s3()\n",
    "\n",
    "print(f\"Successfully uploaded model files to {s3_folder_uri}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading model files to S3...\n",
      "Skipping existing file: s3://bedrock-imported-models-42sjopt3/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/.gitattributes\n",
      "Skipping existing file: s3://bedrock-imported-models-42sjopt3/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/config.json\n",
      "Skipping existing file: s3://bedrock-imported-models-42sjopt3/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/generation_config.json\n",
      "Skipping existing file: s3://bedrock-imported-models-42sjopt3/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/LICENSE\n",
      "Skipping existing file: s3://bedrock-imported-models-42sjopt3/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/model-00001-of-000002.safetensors\n",
      "Skipping existing file: s3://bedrock-imported-models-42sjopt3/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/model-00002-of-000002.safetensors\n",
      "Skipping existing file: s3://bedrock-imported-models-42sjopt3/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/model.safetensors.index.json\n",
      "Skipping existing file: s3://bedrock-imported-models-42sjopt3/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/README.md\n",
      "Skipping existing file: s3://bedrock-imported-models-42sjopt3/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/tokenizer.json\n",
      "Skipping existing file: s3://bedrock-imported-models-42sjopt3/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/tokenizer_config.json\n",
      "Skipping existing file: s3://bedrock-imported-models-42sjopt3/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/figures\\benchmark.jpg\n",
      "Successfully uploaded model files to s3://bedrock-imported-models-42sjopt3/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Deploy the model to Amazon Bedrock\n",
    "### 3.1: Start a model import job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "bedrock = boto3.client('bedrock', region_name=aws_region)\n",
    "model_name = repository_id.split('/')[-1].replace('.', '-').replace('_', '-')\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "job_name = f'{model_name}-{timestamp}'\n",
    "\n",
    "print(f\"Starting model import job: {job_name}\")\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock.create_model_import_job(\n",
    "    jobName=job_name,\n",
    "    importedModelName=model_name,\n",
    "    roleArn=import_role_arn,\n",
    "    modelDataSource={'s3DataSource': {'s3Uri': s3_folder_uri}}\n",
    ")\n",
    "\n",
    "print(f\"Model import job started\")\n",
    "\n",
    "job_arn = response['jobArn']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Monitor the import job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(f\"Checking status of import job: {job_name}\")\n",
    "while True:\n",
    "    response = bedrock.get_model_import_job(jobIdentifier=job_arn)\n",
    "    status = response['status']\n",
    "    if status == 'Failed':\n",
    "        print('Model import failed!')\n",
    "        \n",
    "        failure_message = response['failureMessage']\n",
    "        print(f\"Reason: {failure_message}\")\n",
    "        break\n",
    "    elif status == 'Completed':\n",
    "        print('Model import complete.')\n",
    "        \n",
    "        model_id = response['importedModelArn']\n",
    "        print(f\"Imported model ID: {model_id}\")\n",
    "        break\n",
    "    else:\n",
    "        print('Importing...')\n",
    "\n",
    "    time.sleep(60)  # Check every 60 seconds\n",
    "    \n",
    "model_arn = response['importedModelArn']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: If necessary, wait some more time to make sure the model has been initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for 5 minutes for model initialization \n",
    "print('Waiting 5 minutes for model initialization...')\n",
    "\n",
    "for i in range(5, 0, -1):\n",
    "    print(f'{i} minute{\"s\" if i > 1 else \"\"}...')\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model\n",
    "\n",
    "## Step 1: Define the prompt "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T11:14:15.519769Z",
     "start_time": "2025-02-06T11:14:15.516902Z"
    }
   },
   "source": "prompt = \"Explain the concept of 'rubber duck debugging' in a single paragraph.\"\n",
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Invoke the model\n",
    " \n",
    "### Option 2.1: Using the `InvokeModelWithResponseStream` API\n",
    "This allows you to retrieve and process the response in real-time as chunks.\n",
    "\n",
    "**Pros:** Lower latency to first displayed content; provides visual feedback during generation; reduced memory overhead for large responses\n",
    "\n",
    "**Cons:** More complex implementation; requires handling partial/incomplete responses; need to manage stream state and error handling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T11:09:21.066219Z",
     "start_time": "2025-02-06T11:09:20.309393Z"
    }
   },
   "source": [
    "from botocore.config import Config\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(repository_id)\n",
    "\n",
    "final_response = \"\"\n",
    "\n",
    "def invoke_bedrock_model_stream(model_arn, message, region_name=aws_region, max_tokens=4096):\n",
    "    global final_response\n",
    "    \n",
    "    config = Config(\n",
    "        retries={\n",
    "            'total_max_attempts': 10,\n",
    "            'mode': 'standard'\n",
    "        }\n",
    "    )\n",
    "    session = boto3.session.Session()\n",
    "    br_runtime = session.client(service_name='bedrock-runtime',\n",
    "                                region_name=region_name,\n",
    "                                config=config)\n",
    "    \n",
    "    messages = [{'role': 'user', 'content': message}]\n",
    "    \n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    print(f\"Formatted prompt: \\n{formatted_prompt}\")\n",
    "    \n",
    "    payload = {\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "    response = br_runtime.invoke_model_with_response_stream(\n",
    "        modelId=model_arn,\n",
    "        body=json.dumps(payload)\n",
    "    )\n",
    "    print(\"\\nModel output:\\n\")\n",
    "    for event in response[\"body\"]:\n",
    "        chunk = json.loads(event['chunk']['bytes'])\n",
    "        if \"generation\" in chunk:\n",
    "            text = chunk[\"generation\"]\n",
    "            final_response += text \n",
    "            print(text, end=\"\", flush=True)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T11:14:28.865841Z",
     "start_time": "2025-02-06T11:14:22.873271Z"
    }
   },
   "source": "invoke_bedrock_model_stream(model_arn=model_arn, message=prompt)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted prompt: \n",
      "<｜begin▁of▁sentence｜><｜User｜>Explain the concept of 'rubber duck debugging' in a single paragraph.<｜Assistant｜>\n",
      "\n",
      "Model output:\n",
      "\n",
      "<think>\n",
      "Okay, so I'm trying to understand this concept called \"rubber duck debugging.\" I've heard the term before, but I'm not exactly sure what it means. Let me think about it. I know that \"debugging\" usually means fixing errors in a program or software, right? So maybe \"rubber duck\" has something to do with that process.\n",
      "\n",
      "Wait, rubber duck... I think I've heard it used in a metaphorical sense before. Maybe it's about taking a step back from the problem. Like, when you're stuck on a bug, you try to explain it to someone else, and in doing so, you figure it out yourself. So, if you're working on a computer issue, sometimes you just talk through it with a rubber duck or a stuffed animal, which forces you to articulate the problem clearly.\n",
      "\n",
      "I remember reading somewhere that it's a technique used by software developers. They talk about explaining their code to a rubber duck because it helps them find errors. So, maybe the idea is that by explaining something to another person, even an inanimate object, you get a better grasp of the problem yourself. It's like teaching something to someone else, which makes the concepts clearer.\n",
      "\n",
      "So, putting it all together, rubber duck debugging is a method where you explain the problem to someone or something, which helps you understand it better and identify where you might have gone wrong. It's a way to troubleshoot issues by verbalizing the process, which can make the problem more apparent when you're stuck.\n",
      "\n",
      "I think the key here is the act of explaining. When you break down the problem into words, you have to think through each step logically, which can reveal any misunderstandings or errors you might not have noticed before. It's a simple yet effective technique that helps in debugging by enhancing communication and clarity.\n",
      "</think>\n",
      "\n",
      "Rubber duck debugging is a problem-solving technique used to help identify errors in code or logic. It involves explaining the problem to a rubber duck or another non-human listener, which forces the problem solver to articulate each step clearly. This process helps to clarify the issue, making it easier to spot errors or misunderstandings that might not be obvious when working alone. By verbalizing the problem, the mind becomes more attuned to potential solutions, making it an effective method for enhancing understanding and troubleshooting."
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2.2: Using the `InvokeModel` API\n",
    "This provides a single, complete response after model processing is finished.\n",
    "\n",
    "**Pros:** Simpler implementation; response arrives fully formatted; easier to use for downstream processing\n",
    "\n",
    "**Cons:** Longer perceived wait time; no visual feedback during processing; entire response must be held in memory at once"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T11:13:05.836683Z",
     "start_time": "2025-02-06T11:13:05.831349Z"
    }
   },
   "source": [
    "def invoke_model(model, message, region_name=aws_region, max_tokens=4096):\n",
    "    config = Config(\n",
    "        retries={\n",
    "            'total_max_attempts': 10, \n",
    "            'mode': 'standard'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    session = boto3.session.Session()\n",
    "    br_runtime = session.client('bedrock-runtime', region_name=region_name, config=config)\n",
    "    \n",
    "    messages = [{'role': 'user', 'content': message}]\n",
    "    \n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    print(f\"Formatted prompt:\\n{formatted_prompt}\")\n",
    "    \n",
    "    payload = {\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "        \n",
    "    try:\n",
    "        response = br_runtime.invoke_model(\n",
    "            modelId=model, \n",
    "            body=json.dumps(payload) \n",
    "        )\n",
    "        result = json.loads(response[\"body\"].read().decode(\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(e.__repr__())\n",
    "\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T11:14:44.616357Z",
     "start_time": "2025-02-06T11:14:39.003583Z"
    }
   },
   "cell_type": "code",
   "source": "response = invoke_model(model=model_arn, message=prompt)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted prompt:\n",
      "<｜begin▁of▁sentence｜><｜User｜>Explain the concept of 'rubber duck debugging' in a single paragraph.<｜Assistant｜>\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T11:14:47.799693Z",
     "start_time": "2025-02-06T11:14:47.794644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython import display\n",
    "display.Markdown(response['generation'])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "<think>\nOkay, so I need to explain the concept of 'rubber duck debugging' in a single paragraph. I've heard the term before, but I'm not exactly sure what it means. Let me try to break it down. \n\nFirst, I think it's related to programming or software development because the term 'debugging' suggests fixing errors. Rubber duck debugging sounds like it's a specific method for debugging. Maybe it's a technique where you explain your problem to a rubber duck? That seems a bit odd because a duck isn't going to understand programming. But perhaps it's a metaphor for a process.\n\nI remember hearing that sometimes when you're stuck on a problem, explaining it to someone else can help you figure it out yourself. So maybe rubber duck debugging is similar. Instead of a person, you use a rubber duck as a dummy audience. By explaining the problem out loud or to an object, the act of verbalizing the issue can help clarify your own thoughts. That makes sense because sometimes when you talk through a problem, you catch your own mistakes or see a solution.\n\nI should also consider the origins of the term. I think it's a practice used by programmers. They might use a rubber duck as a prop to simulate explaining the code to someone else, like a colleague or a rubber duck. This way, they can test their own understanding and identify where they're getting stuck.\n\nSo, putting it all together, rubber duck debugging is a technique where you explain a programming problem to a rubber duck or an imaginary audience. By doing so, you can better understand the problem and find the solution. It's a form of debugging that relies on verbal communication to enhance problem-solving.\n</think>\n\nRubber duck debugging is a technique used in programming and problem-solving where one explains a problem to a rubber duck or an imaginary audience. By verbalizing the issue, often through storytelling, individuals can gain clarity and identify their own misunderstandings or errors, facilitating a better grasp of the problem and leading to a potential solution. This practice leverages the power of communication to enhance self-discovery and critical thinking."
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
