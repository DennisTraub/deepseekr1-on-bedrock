{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "    DeepSeek-R1 with Amazon Bedrock \n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the model weights (`DeepSeek-R1-Distill-Qwen-14B`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Download the model weights locally from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fc00d84a684d54be80f93ee4c68fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded to: /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Model ID\n",
    "repo_id = 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B'\n",
    "\n",
    "# Downloading the model weights locally from Hugging Face\n",
    "local_dir = snapshot_download(repo_id)\n",
    "print(f\"Model downloaded to: {local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Upload the weights on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/config.json to s3://llm-weights-demo/models/DeepSeek-R1-Distill-Llama-8B/config.json\n",
      "Uploading /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/README.md to s3://llm-weights-demo/models/DeepSeek-R1-Distill-Llama-8B/README.md\n",
      "Uploading /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/LICENSE to s3://llm-weights-demo/models/DeepSeek-R1-Distill-Llama-8B/LICENSE\n",
      "Uploading /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/generation_config.json to s3://llm-weights-demo/models/DeepSeek-R1-Distill-Llama-8B/generation_config.json\n",
      "Uploading /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/model.safetensors.index.json to s3://llm-weights-demo/models/DeepSeek-R1-Distill-Llama-8B/model.safetensors.index.json\n",
      "Uploading /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/.gitattributes to s3://llm-weights-demo/models/DeepSeek-R1-Distill-Llama-8B/.gitattributes\n",
      "Uploading /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/tokenizer_config.json to s3://llm-weights-demo/models/DeepSeek-R1-Distill-Llama-8B/tokenizer_config.json\n",
      "Uploading /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/tokenizer.json to s3://llm-weights-demo/models/DeepSeek-R1-Distill-Llama-8B/tokenizer.json\n",
      "Uploading /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/model-00002-of-000002.safetensors to s3://llm-weights-demo/models/DeepSeek-R1-Distill-Llama-8B/model-00002-of-000002.safetensors\n",
      "Uploading /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/model-00001-of-000002.safetensors to s3://llm-weights-demo/models/DeepSeek-R1-Distill-Llama-8B/model-00001-of-000002.safetensors\n",
      "Uploading /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/figures/benchmark.jpg to s3://llm-weights-demo/models/DeepSeek-R1-Distill-Llama-8B/figures/benchmark.jpg\n",
      "Model uploaded to S3 successfully.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# Uploading the model files to the specified S3 bucket\n",
    "bucket_name = 'llm-weights-demo'\n",
    "target_folder = 'models/DeepSeek-R1-Distill-Llama-8B'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "for root, _, files in os.walk(local_dir):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        relative_path = os.path.relpath(file_path, local_dir)\n",
    "        s3_key = f\"{target_folder}/{relative_path}\"  \n",
    "        print(f\"Uploading {file_path} to s3://{bucket_name}/{s3_key}\")\n",
    "        s3.upload_file(file_path, bucket_name, s3_key)\n",
    "\n",
    "print(\"Model uploaded to S3 successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the model on Amazon Bedrock "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Go to the `AWS console` and, on the left-hand size, click on **Imported models under Foundation models**.\n",
    "- Click on **Import model**\n",
    "- Use `my-DeepSeek-R1-Distill-Llama-8B` as the Model name, and enter the S3 location from above. Click on Import model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img1](imgs/img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wait for the model to get import **sucessfully**\n",
    "\n",
    "  ![img2](imgs/img2.png)\n",
    "\n",
    "- Take a note of the **Model ARN**\n",
    "\n",
    "  ![img3](imgs/img3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "import json\n",
    "\n",
    "def invoke_bedrock_model(model_arn, message, region_name='us-east-1'):\n",
    "\n",
    "    model_id = model_arn\n",
    "\n",
    "    config = Config(\n",
    "        retries={\n",
    "            'total_max_attempts': 10, \n",
    "            'mode': 'standard'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    session = boto3.session.Session()\n",
    "    br_runtime = session.client(service_name = 'bedrock-runtime', \n",
    "                                     region_name=region_name, \n",
    "                                     config=config)\n",
    "        \n",
    "    try:\n",
    "        invoke_response = br_runtime.invoke_model(modelId=model_id, \n",
    "                                                body=json.dumps({'prompt': message}), \n",
    "                                                accept=\"application/json\", \n",
    "                                                contentType=\"application/json\")\n",
    "        result = invoke_response[\"body\"] = json.loads(invoke_response[\"body\"].read().decode(\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(e.__repr__())\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arn = 'arn:aws:bedrock:us-east-1:507922848584:imported-model/0j54azgqtt5l'\n",
    "\n",
    "prompt = \"A bat and a ball together cost $10.50. The bat costs $2.25 more than the ball. How much does the ball cost?\"\n",
    "\n",
    "response = invoke_bedrock_model(model_arn=model_arn, message=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " \n",
       "\n",
       "First, let me define the variables. Letâ€™s say the cost of the ball is \\( b \\). Then, the cost of the bat would be \\( b + 2.25 \\).\n",
       "\n",
       "Since the total cost is $10.50, I can set up the equation: \\( b + (b + 2.25) = 10.50 \\).\n",
       "\n",
       "Combining like terms, this becomes \\( 2b + 2.25 = 10.50 \\).\n",
       "\n",
       "Subtracting 2.25 from both sides gives \\( 2b = 8.25 \\).\n",
       "\n",
       "Dividing both sides by 2, I find that \\( b = 4.125 \\).\n",
       "\n",
       "So, the ball costs $4.125.\n",
       "</think>\n",
       "\n",
       "Let's solve the problem step by step.\n",
       "\n",
       "**Given:**\n",
       "- The total cost of a bat and a ball together is \\$10.50.\n",
       "- The bat costs \\$2.25 more than the ball.\n",
       "\n",
       "**Let:**\n",
       "- \\( b \\) = Cost of the ball (\\$)\n",
       "- \\( b + 2.25 \\) = Cost of the bat (\\$)\n",
       "\n",
       "**Set up the equation:**\n",
       "\\[\n",
       "b + (b + 2.25) = 10.50\n",
       "\\]\n",
       "\n",
       "**Combine like terms:**\n",
       "\\[\n",
       "2b + 2.25 = 10.50\n",
       "\\]\n",
       "\n",
       "**Subtract 2.25 from both sides:**\n",
       "\\[\n",
       "2b = 10.50 - 2.25\n",
       "\\]\n",
       "\\[\n",
       "2b = 8.25\n",
       "\\]\n",
       "\n",
       "**Divide both sides by 2:**\n",
       "\\[\n",
       "b = \\frac{8.25}{2}\n",
       "\\]\n",
       "\\[\n",
       "b = 4.125\n",
       "\\]\n",
       "\n",
       "**Answer:**\n",
       "The ball costs \\(\\boxed{4.125}\\) dollars."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "display.Markdown(response['generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
