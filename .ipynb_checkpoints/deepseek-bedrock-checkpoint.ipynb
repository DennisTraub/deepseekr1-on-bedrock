{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "    DeepSeek-R1 with Amazon Bedrock \n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "- An AWS account with access to Amazon Bedrock\n",
    "- Sufficient local storage space (at least 17GB for 8B and 135GB for 70B models)\n",
    "- (Optional) An Amazon S3 bucket prepared to store the custom model\n",
    "- (Optional) An AWS IAM Role with permissions for Bedrock to read from S3\n",
    "\n",
    "## Step 1: Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
      "The folder you are executing pip from can no longer be found.\n",
      "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
      "The folder you are executing pip from can no longer be found.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U huggingface_hub -q\n",
    "!pip install boto3 --upgrade -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure parameters\n",
    "Note: When using the defaults, unique random strings will be appended to resource names to prevent naming conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Hugging Face repository ID ['deepseek-ai/DeepSeek-R1-Distill-Llama-8B']:  \n",
      "Enter the AWS region: ['us-east-1']:  \n",
      "Enter the IAM role name for the model import [Leave empty to create a new role] \n",
      "Enter the S3 bucket name [Leave empty to create a new bucket] llm-weights-demo\n",
      "Enter the S3 root prefix ['/'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "- HF Repository ID: deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
      "- Import role ARN: Create a new IAM role\n",
      "- S3 bucket: llm-weights-demo\n",
      "- S3 root folder: /\n"
     ]
    }
   ],
   "source": [
    "# Default configuration values\n",
    "default_region = 'us-east-1'\n",
    "default_repository_id = 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B'\n",
    "default_s3_root_folder = '/'\n",
    "default_s3_bucket_base_name = 'bedrock-imported-models'\n",
    "default_import_role_base_name = 'AmazonBedrockModelImportRole'\n",
    "default_import_policy_name = 'AmazonBedrockModelImportPolicy'\n",
    "\n",
    "# Collect required parameters from user\n",
    "# Allow user to specify custom Hugging Face model repository\n",
    "repository_id = input(f\"Enter Hugging Face repository ID ['{default_repository_id}']: \") or default_repository_id\n",
    "\n",
    "# Allow user to specify AWS region for deployment\n",
    "aws_region = input(f\"Enter the AWS region: ['us-east-1']: \") or default_region\n",
    "\n",
    "# Get IAM role name for model import permissions\n",
    "# If left empty, a new role will be created automatically\n",
    "import_role_name = input(\"Enter the IAM role name for the model import [Leave empty to create a new role]\") or None\n",
    "\n",
    "# Get S3 storage configuration\n",
    "# If left is empty, a new bucket will be created\n",
    "s3_bucket_name = input('Enter the S3 bucket name [Leave empty to create a new bucket]') or None\n",
    "s3_root_folder = input(f\"Enter the S3 root prefix ['{default_s3_root_folder}']\") or default_s3_root_folder\n",
    "\n",
    "# Display final configuration settings\n",
    "print('Configuration:')\n",
    "print(f\"- HF Repository ID: {repository_id}\")\n",
    "print(f\"- Import role ARN: {import_role_name or 'Create a new IAM role'}\")\n",
    "print(f\"- S3 bucket: {s3_bucket_name or 'Create a new S3 bucket'}\")\n",
    "print(f\"- S3 root folder: {s3_root_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create new IAM Role and S3 Bucket if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new IAM role...\n",
      "Successfully created IAM role: AmazonBedrockModelImportRole-nzgkim9p\n",
      "Checking S3 bucket: llm-weights-demo\n",
      "Found existing bucket\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a random resource name postfix\n",
    "postfix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))\n",
    "\n",
    "def get_aws_account_id():\n",
    "    sts_client = boto3.client('sts')\n",
    "    return sts_client.get_caller_identity()['Account']\n",
    "\n",
    "def get_or_create_role(role_name):\n",
    "    iam_client = boto3.client('iam')\n",
    "    \n",
    "    if not role_name:\n",
    "        print('Creating new IAM role...')\n",
    "        \n",
    "        account_id = get_aws_account_id()\n",
    "        role_name = f\"{default_import_role_base_name}-{postfix}\"\n",
    "       \n",
    "        trust_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [{\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": { \"Service\": \"bedrock.amazonaws.com\" },\n",
    "                \"Action\": \"sts:AssumeRole\",\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": { \"aws:SourceAccount\": account_id },\n",
    "                    \"ArnEquals\": {\n",
    "                        \"aws:SourceArn\": f\"arn:aws:bedrock:{aws_region}:{account_id}:model-import-job/*\"\n",
    "                    }\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        inline_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [{\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": [\n",
    "                        \"s3:GetObject\",\n",
    "                        \"s3:ListBucket\"\n",
    "                    ],\n",
    "                    \"Resource\": [\n",
    "                        s3_bucket_arn,\n",
    "                        f\"{s3_bucket_arn}/*\"\n",
    "                    ],\n",
    "                    \"Condition\": {\n",
    "                        \"StringEquals\": {\n",
    "                            \"aws:ResourceAccount\": account_id\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            role = iam_client.create_role(\n",
    "                RoleName=role_name,\n",
    "                AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
    "            )\n",
    "\n",
    "            iam_client.put_role_policy(\n",
    "                RoleName=role_name,\n",
    "                PolicyName=f\"{default_import_policy_name}\",\n",
    "                PolicyDocument=json.dumps(inline_policy)\n",
    "            )\n",
    "            \n",
    "            print(f\"Successfully created IAM role: {role_name}\")\n",
    "        except ClientError as e:\n",
    "            print(f\"Error creating IAM role: {e}\")\n",
    "            exit(1)\n",
    "    else:\n",
    "        print(f\"Checking IAM role: {role_name}\")\n",
    "        try:\n",
    "            role = iam_client.get_role(RoleName=role_name)\n",
    "            print('Found existing role.')\n",
    "        except ClientError as e:\n",
    "            print(f\"Error retrieving S3 bucket: {e}\")\n",
    "            exit(1)\n",
    "            \n",
    "    return role[\"Role\"]\n",
    "\n",
    "def get_or_create_bucket(bucket_name):\n",
    "    s3_client = boto3.client('s3', region_name=aws_region)\n",
    "    \n",
    "    if not bucket_name:\n",
    "        print(f\"Creating new S3 bucket...\")\n",
    "        \n",
    "        bucket_name = f\"{default_s3_bucket_base_name}-{postfix}\"\n",
    "        \n",
    "        try:\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "            \n",
    "            # Wait until bucket exists\n",
    "            waiter = s3_client.get_waiter('bucket_exists')\n",
    "            waiter.wait(\n",
    "                Bucket=bucket_name,\n",
    "                WaiterConfig={\n",
    "                    'Delay': 5,\n",
    "                    'MaxAttempts': 20\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            print(f\"Successfully created S3 bucket: {bucket_name}\")\n",
    "            return bucket_name\n",
    "            \n",
    "        except ClientError as e:\n",
    "            print(f\"Error creating S3 bucket: {e}\")\n",
    "            exit(1)\n",
    "    else:\n",
    "        print(f\"Checking S3 bucket: {bucket_name}\")\n",
    "        try:\n",
    "            bucket = s3_client.head_bucket(Bucket=bucket_name)\n",
    "            print('Found existing bucket')\n",
    "        except ClientError as e:\n",
    "            print(f\"Error retrieving IAM role: {e}\")\n",
    "            exit(1)\n",
    "            \n",
    "s3_bucket_arn = f\"arn:aws:s3:::{s3_bucket_name}\"\n",
    "import_role = get_or_create_role(import_role_name)\n",
    "import_role_arn = import_role[\"Arn\"]\n",
    "\n",
    "s3_bucket = get_or_create_bucket(s3_bucket_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Download and deploy the model\n",
    "## Step 1: Download the weights from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892d0faa168446738561fc91430c9c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded to: /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_dir = snapshot_download(repository_id)\n",
    "print(f\"Model downloaded to: {local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload the weights to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading model files to S3...\n",
      "Uploading: /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/config.json to s3://llm-weights-demo/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/config.json\n",
      "Uploading: /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/README.md to s3://llm-weights-demo/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/README.md\n",
      "Uploading: /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/LICENSE to s3://llm-weights-demo/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/LICENSE\n",
      "Uploading: /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/generation_config.json to s3://llm-weights-demo/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/generation_config.json\n",
      "Uploading: /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/model.safetensors.index.json to s3://llm-weights-demo/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/model.safetensors.index.json\n",
      "Uploading: /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/.gitattributes to s3://llm-weights-demo/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/.gitattributes\n",
      "Uploading: /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/tokenizer_config.json to s3://llm-weights-demo/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/tokenizer_config.json\n",
      "Uploading: /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/tokenizer.json to s3://llm-weights-demo/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/tokenizer.json\n",
      "Uploading: /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/model-00002-of-000002.safetensors to s3://llm-weights-demo/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/model-00002-of-000002.safetensors\n",
      "Uploading: /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/model-00001-of-000002.safetensors to s3://llm-weights-demo/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/model-00001-of-000002.safetensors\n",
      "Uploading: /home/sagemaker-user/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/24ae87a9c340aa4207dd46509414c019998e0161/figures/benchmark.jpg to s3://llm-weights-demo/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/figures/benchmark.jpg\n",
      "Successfully uploaded model files to s3://llm-weights-demo/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "s3_folder = repository_id if s3_root_folder == '/' else f'{s3_root_folder}/{repository_id}'\n",
    "s3_folder_uri = f\"s3://{s3_bucket_name}/{s3_folder}\"\n",
    "\n",
    "def file_exists_in_s3(bucket_name, s3_key):\n",
    "    return s3.list_objects_v2(Bucket=bucket_name, Prefix=s3_key)['KeyCount'] > 0\n",
    "\n",
    "def upload_to_s3():\n",
    "    for root, _, files in os.walk(local_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(file_path, local_dir)\n",
    "            s3_key = f\"{s3_folder}/{relative_path}\"\n",
    "            \n",
    "            if file_exists_in_s3(s3_bucket_name, s3_key):\n",
    "                print(f\"Skipping existing file: s3://{s3_bucket_name}/{s3_key}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Uploading: {file_path} to s3://{s3_bucket_name}/{s3_key}\")\n",
    "            s3.upload_file(file_path, s3_bucket_name, s3_key)\n",
    "\n",
    "print('Uploading model files to S3...')\n",
    "\n",
    "upload_to_s3()\n",
    "\n",
    "print(f\"Successfully uploaded model files to {s3_folder_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Deploy the model to Amazon Bedrock\n",
    "### 3.1: Start a model import job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model import job: DeepSeek-R1-Distill-Llama-8B-20250129150919\n",
      "Model import job started\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "bedrock = boto3.client('bedrock', region_name=aws_region)\n",
    "model_name = repository_id.split('/')[-1].replace('.', '-').replace('_', '-')\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "job_name = f'{model_name}-{timestamp}'\n",
    "\n",
    "print(f\"Starting model import job: {job_name}\")\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock.create_model_import_job(\n",
    "    jobName=job_name,\n",
    "    importedModelName=model_name,\n",
    "    roleArn=import_role_arn,\n",
    "    modelDataSource={'s3DataSource': {'s3Uri': s3_folder_uri}}\n",
    ")\n",
    "\n",
    "print(f\"Model import job started\")\n",
    "\n",
    "job_arn = response['jobArn']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Monitor the import job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking status of import job: DeepSeek-R1-Distill-Llama-8B-20250129150919\n",
      "Importing...\n",
      "Importing...\n",
      "Importing...\n",
      "Importing...\n",
      "Importing...\n",
      "Importing...\n",
      "Importing...\n",
      "Model import complete.\n",
      "Imported model ID: arn:aws:bedrock:us-east-1:507922848584:imported-model/c5kq0vzsdrms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(f\"Checking status of import job: {job_name}\")\n",
    "while True:\n",
    "    response = bedrock.get_model_import_job(jobIdentifier=job_arn)\n",
    "    status = response['status']\n",
    "    if status == 'Failed':\n",
    "        print('Model import failed!')\n",
    "        \n",
    "        failure_message = response['failureMessage']\n",
    "        print(f\"Reason: {failure_message}\")\n",
    "        break\n",
    "    elif status == 'Completed':\n",
    "        print('Model import complete.')\n",
    "        \n",
    "        model_id = response['importedModelArn']\n",
    "        print(f\"Imported model ID: {model_id}\")\n",
    "        break\n",
    "    else:\n",
    "        print('Importing...')\n",
    "\n",
    "    time.sleep(60)  # Check every 60 seconds\n",
    "    \n",
    "model_arn = response['importedModelArn']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: If necessary, wait some more time to make sure the model has been initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 5 minutes for model initialization...\n",
      "5 minutes...\n",
      "4 minutes...\n",
      "3 minutes...\n",
      "2 minutes...\n",
      "1 minute...\n"
     ]
    }
   ],
   "source": [
    "# Wait for 5 minutes for model initialization \n",
    "print('Waiting 5 minutes for model initialization...')\n",
    "\n",
    "for i in range(5, 0, -1):\n",
    "    print(f'{i} minute{\"s\" if i > 1 else \"\"}...')\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Test the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A train leaves Station A at 9:00 AM, traveling at 60 km/h. Another train leaves Station B, 120 km away, at 10:00 AM, traveling at 80 km/h toward Station A. At what time will they meet?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 2: Using `Streaming` API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_bedrock_model_stream(model_arn, message, region_name=aws_region, max_tokens=4096):\n",
    "    config = Config(\n",
    "        retries={\n",
    "            'total_max_attempts': 10,\n",
    "            'mode': 'standard'\n",
    "        }\n",
    "    )\n",
    "    session = boto3.session.Session()\n",
    "    br_runtime = session.client(service_name='bedrock-runtime',\n",
    "                                region_name=region_name,\n",
    "                                config=config)\n",
    "    payload = {\n",
    "        \"prompt\": message,\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "    response = br_runtime.invoke_model_with_response_stream(\n",
    "        modelId=model_arn,\n",
    "        body=json.dumps(payload),\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    print(\"Model output:\\n\")\n",
    "    for event in response[\"body\"]:\n",
    "        chunk = json.loads(event['chunk']['bytes'])\n",
    "        if \"generation\" in chunk:\n",
    "            print(chunk[\"generation\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output:\n",
      "\n",
      " How far from Station A will they meet?\n",
      "Okay, so I have this problem here about two trains leaving different stations and heading towards each other. I need to figure out when and where they'll meet. Let me break it down step by step.\n",
      "\n",
      "First, let's list out the given information:\n",
      "\n",
      "- Train 1 leaves Station A at 9:00 AM, traveling at 60 km/h.\n",
      "- Station A and Station B are 120 km apart.\n",
      "- Train 2 leaves Station B at 10:00 AM, traveling at 80 km/h towards Station A.\n",
      "\n",
      "I need to find two things:\n",
      "\n",
      "1. The time when they will meet.\n",
      "2. The distance from Station A where they will meet.\n",
      "\n",
      "Alright, so let me visualize this scenario. Station A and Station B are 120 km apart. Train 1 starts from A at 9:00 AM, going towards B at 60 km/h. Train 2 starts from B an hour later, at 10:00 AM, moving towards A at 80 km/h. They're moving towards each other, so their speeds will add up when calculating how quickly they'll close the distance between them.\n",
      "\n",
      "Hmm, okay. So, maybe I can think of this as a relative speed problem. When two objects move towards each other, their relative speed is the sum of their individual speeds. That makes sense because they're getting closer faster that way.\n",
      "\n",
      "So, let's compute the relative speed first.\n",
      "\n",
      "Train 1's speed = 60 km/h\n",
      "Train 2's speed = 80 km/h\n",
      "Relative speed = 60 + 80 = 140 km/h\n",
      "\n",
      "So, together, they're closing the distance at 140 km/h.\n",
      "\n",
      "Now, the distance between the two stations is 120 km. So, the time it takes for them to meet should be the distance divided by their relative speed, right?\n",
      "\n",
      "Time = Distance / Speed = 120 km / 140 km/h\n",
      "\n",
      "Let me compute that. 120 divided by 140. Hmm, 120/140 simplifies to 6/7 hours. 6 divided by 7 is approximately 0.857 hours.\n",
      "\n",
      "To convert 0.857 hours into minutes, since the departure times are given in hours, I can multiply by 60 minutes per hour.\n",
      "\n",
      "0.857 * 60 ≈ 51.43 minutes.\n",
      "\n",
      "So, approximately 51.43 minutes after one of the trains starts, they'll meet.\n",
      "\n",
      "But wait, the second train doesn't start until 10"
     ]
    }
   ],
   "source": [
    "invoke_bedrock_model_stream(model_arn=model_arn,\n",
    "                            message=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 3 (Optional): Using `Invoke Model` API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "\n",
    "def invoke_model(model, message, max_tokens=4096):\n",
    "    config = Config(\n",
    "        retries={\n",
    "            'total_max_attempts': 10, \n",
    "            'mode': 'standard'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    inf_params = {\"max_new_tokens\": 300, \"top_p\": 0.9, \"top_k\": 20}\n",
    "\n",
    "    session = boto3.session.Session()\n",
    "    br_runtime = session.client('bedrock-runtime', region_name=aws_region, config=config)\n",
    "        \n",
    "    try:\n",
    "        invoke_response = br_runtime.invoke_model(\n",
    "            modelId=model, \n",
    "            body=json.dumps({'prompt': message, \n",
    "                            \"max_tokens\": max_tokens}) \n",
    "        )\n",
    "        result = invoke_response[\"body\"] = json.loads(invoke_response[\"body\"].read().decode(\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(e.__repr__())\n",
    "\n",
    "    return result\n",
    "\n",
    "response = invoke_model(model=model_arn, message=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " \n",
       "\n",
       "First, I need to figure out how much time each train has been traveling when they meet. Let me denote the time after 10:00 AM as \\( t \\) hours.\n",
       "\n",
       "For Train A:\n",
       "- It leaves at 9:00 AM, so by 10:00 AM, it has already traveled 1 hour.\n",
       "- In the next \\( t \\) hours, it will travel an additional 60t km.\n",
       "- So, the total distance covered by Train A when they meet is \\( 60t + 60 \\) km.\n",
       "\n",
       "For Train B:\n",
       "- It leaves at 10:00 AM, so it travels for \\( t \\) hours.\n",
       "- In that time, it will cover \\( 80t \\) km.\n",
       "- Since Station B is 120 km away from Station A, the distance covered by Train B is \\( 80t \\) km.\n",
       "\n",
       "When they meet, the total distance covered by both trains should add up to 120 km. So, I can set up the equation:\n",
       "\\( 60t + 60 + 80t = 120 \\)\n",
       "\n",
       "Combining like terms:\n",
       "\\( 140t + 60 = 120 \\)\n",
       "\n",
       "Subtracting 60 from both sides:\n",
       "\\( 140t = 60 \\)\n",
       "\n",
       "Dividing both sides by 140:\n",
       "\\( t = \\frac{60}{140} \\)\n",
       "\\( t = \\frac{6}{14} \\)\n",
       "\\( t = \\frac{3}{7} \\) hours\n",
       "\n",
       "Now, converting \\( \\frac{3}{7} \\) hours to minutes:\n",
       "\\( \\frac{3}{7} \\times 60 =  43.5714 \\) minutes, which is approximately 43 minutes and 34 seconds.\n",
       "\n",
       "Adding this time to 10:00 AM:\n",
       "10:00 AM + 43 minutes and 34 seconds = 10:43:34 AM.\n",
       "\n",
       "So, the trains will meet at approximately 10:43 AM.\n",
       "\\\n",
       "\n",
       "**Step-by-Step Explanation:**\n",
       "\n",
       "1. **Define Variables:**\n",
       "   - Let \\( t \\) be the time in hours after 10:00 AM when the two trains meet.\n",
       "\n",
       "2. **Calculate Distance Covered by Each Train:**\n",
       "   - **Train A:** Left at 9:00 AM, so by 10:00 AM, it has already traveled 1 hour. In the next \\( t \\) hours, it will cover \\( 60t \\) km. Total distance covered by Train A when they"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "display.Markdown(response['generation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
